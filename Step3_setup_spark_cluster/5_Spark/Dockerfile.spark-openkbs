FROM openkbs/tensorflow-python3-jupyter
#FROM ubuntu18.04
#FROM 11.4.1-devel-ubuntu20.04 
#11.4.1-base-ubuntu18.04

MAINTAINER OpenKBS <DrSnowbird@openkbs.org>

USER ${USER}
WORKDIR ${HOME}

#### ---- Install some Tensorflow dependencies ----
RUN sudo apt-get update -y && \
    sudo apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    libfreetype6-dev \
    libpng-dev \
    libzmq3-dev \
    pkg-config \
    python \
    python3 \
    python3-dev \
    python-pip \
    python3-pip \
    python3-setuptools \
    rsync \
    software-properties-common \
    unzip \
    pkg-config libpcre3-dev zlib1g-dev liblzma-dev \
    && \
    sudo apt-get clean && \
    sudo rm -rf /var/lib/apt/lists/*

#### Graphviz install ####
#temp gpg error fix
RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -
#temp gpg error fix
RUN sudo apt-get update -y
RUN sudo apt-get install -y graphviz

###################################
#### ----   PIP modules: ----  ####
###################################
#### ---- pip3 Package installation ---- ####

COPY config/requirements.txt ./
ENV PATH="$HOME/.local/bin:$PATH"
RUN sudo python3 -m pip --no-cache-dir install --upgrade pip && \
    sudo python3 -m pip --no-cache-dir install --upgrade setuptools tensorflow && \
    sudo python3 -m pip --no-cache-dir install --ignore-installed notebook && \
    sudo python3 -m pip --no-cache-dir install --ignore-installed -r requirements.txt

RUN sudo python3 -m ipykernel.kernelspec

##################################
#### ----   Virtualenv: ----  ####
##################################

RUN sudo apt-get update -y && \
    sudo apt-get install python3-dev python3-pip && \
    sudo -H pip3 --no-cache-dir install -U virtualenv 
    
#### ---- Ref: https://www.tensorflow.org/install/pip ---- ####
RUN virtualenv --system-site-packages -p python3 ./venv && \
    . ./venv/bin/activate  # sh, bash, ksh, or zsh && \
    pip install --upgrade pip && \
    pip list  # show packages installed within the virtual environment
    
RUN sudo chown -R ${USER}:${USER} ${HOME}

## -- added Local PIP installation bin to PATH
ENV PATH=${PATH}:${HOME}/.local/bin

#### ---- R inside Jupyter ---- ####
#RUN sudo apt install -y dirmngr gnupg apt-transport-https ca-certificates software-properties-common && \
#RUN sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/'  && \
RUN sudo apt-get install -y r-base  && \
    sudo apt-get update -y  && \
    sudo apt-get install -y python3-ipykernel

##################################
#### Set up user environments ####
##################################

RUN echo "USER =======> ${USER}"

ENV WORKSPACE=${HOME}/workspace
ENV DATA=${HOME}/data

RUN mkdir -p ${WORKSPACE} ${DATA} 

#### ---- Jupyter Notebook Extensions ---- ####
USER root

RUN apt-get update -y && apt-get dist-upgrade -y
RUN pip install --user jupyter_contrib_nbextensions && \
    jupyter contrib nbextension install

# Expose Ports for TensorBoard (6006), Ipython (8888)
EXPOSE 6006
EXPOSE 8888

VOLUME $HOME/data
VOLUME $HOME/workspace
VOLUME $HOME/logs
VOLUME $HOME/notebooks

## ref: https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(Native-GPU-Support)#usage
##
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,video,utility

############################################################################################
#FROM openkbs/tensorflow-python3-jupyter:latest
############################################################################################

RUN apt install openjdk-8-jre-headless

COPY ./config/install-hadoop.sh .
RUN bash install-hadoop.sh

COPY ./config/install-spark.sh .
RUN bash install-spark.sh

COPY ./config/spark-defaults.conf /usr/local/spark/conf/
COPY ./config/spark-slaves /usr/local/spark/conf/slaves
COPY ./config/spark-env.sh /usr/local/spark/conf/spark-env.sh

COPY ./config/hadoop-workers /opt/hadoop/etc/hadoop/workers
COPY ./config/hadoop-slaves /opt/hadoop/etc/hadoop/slaves
COPY ./config/hadoop-env.sh /opt/hadoop/etc/hadoop/hadoop-env.sh

COPY ./config/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
COPY ./config/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
COPY ./config/mapred-site.xml /opt/hadoop/etc/hadoop/mapred-site.xml
COPY ./config/yarn-site.xml /opt/hadoop/etc/hadoop/yarn-site.xml

COPY ./config/hosts /etc/hosts

COPY ./config/install-openssh-debian.sh .
RUN bash ./install-openssh-debian.sh

#### Download a postgresql jar file and copy it to /opt/hive/lib/
RUN wget https://jdbc.postgresql.org/download/postgresql-42.2.24.jar --no-check-certificate -O /usr/local/spark/jars/postgresql-42.2.24.jar 

## Install  MongoDB JDBC jar in Spark jars folder.
RUN wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.4.0/mongo-spark-connector_2.11-2.4.0.jar --no-check-certificate -O /usr/local/spark/jars/mongo-spark-connector_2.11-2.4.0.jar

## Install  MySQL jar in Spark jars folder.
RUN wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar --no-check-certificate -O /usr/local/spark/jars/mysql-connector-java-8.0.28.jar

RUN chown -R ${USER} ${HOME}
USER ${USER}
WORKDIR "$HOME"
