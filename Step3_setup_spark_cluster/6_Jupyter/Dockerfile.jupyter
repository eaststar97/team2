FROM continuumio/anaconda3 as conda

FROM jungfrau70/centos7:ansible.1

USER root
RUN yum install -y java-1.8.0-openjdk-devel
RUN yum install -y which

COPY ./config/install-hadoop.sh .
RUN bash install-hadoop.sh

COPY ./config/install-spark.sh .
RUN bash install-spark.sh

COPY ./config/spark-defaults.conf /usr/local/spark/conf/
COPY ./config/spark-slaves /usr/local/spark/conf/slaves
COPY ./config/spark-env.sh /usr/local/spark/conf/spark-env.sh

COPY ./config/hadoop-workers /opt/hadoop/etc/hadoop/workers
COPY ./config/hadoop-slaves /opt/hadoop/etc/hadoop/slaves
COPY ./config/hadoop-env.sh /opt/hadoop/etc/hadoop/hadoop-env.sh

COPY ./config/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
COPY ./config/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
COPY ./config/mapred-site.xml /opt/hadoop/etc/hadoop/mapred-site.xml
COPY ./config/yarn-site.xml /opt/hadoop/etc/hadoop/yarn-site.xml

## Install  MongoDB JDBC jar in Spark jars folder.
RUN wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.4.0/mongo-spark-connector_2.11-2.4.0.jar --no-check-certificate -O /usr/local/spark/jars/mongo-spark-connector_2.11-2.4.0.jar

COPY ./config/hosts /etc/hosts

ENV SHARED_WORKSPACE=/opt/workspace 

COPY --from=conda /opt/conda/ /opt/conda/
RUN /opt/conda/bin/conda install python=3.6
#RUN yum install -y unzip
#COPY ./config/install-python3.sh .
#RUN bash install-python3.sh

COPY ./config/requirements.txt .
RUN pip3 install -r requirements.txt

WORKDIR ${SHARED_WORKSPACE}

#CMD jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=